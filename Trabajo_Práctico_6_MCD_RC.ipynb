{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matemática para Ciencia de los Datos\n",
    "# Trabajo Práctico 6\n",
    "\n",
    "Profesor: Luis Alexánder Calvo Valverde \n",
    "\n",
    "Instituto Tecnológico de Costa Rica, \n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: Lunes 5 de Junio del 2023, a más tardar a las 3:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ). \n",
    "\n",
    "Estudiante:\n",
    "1. **Ricardo Chacon Brenes**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n",
    "variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por: \n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n",
    "la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la función: \n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la función en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el máximo número de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n",
    "\n",
    "**Nota:** \n",
    "1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def funcion_SGD(tasa_aprendizaje, iteraciones, xy, tolerancia):\n",
    "    x, y = xy\n",
    "    \n",
    "    for i in range(iteraciones):\n",
    "        # Calcular las derivadas\n",
    "        df_dx = 2 * x\n",
    "        df_dy = -2 * y\n",
    "        \n",
    "        # Calcular los nuevos valores de x y y\n",
    "        new_x = x + tasa_aprendizaje * df_dx\n",
    "        new_y = y + tasa_aprendizaje * df_dy\n",
    "        \n",
    "        # Comprobar la condición de tolerancia\n",
    "        if np.sqrt((new_x - x)**2 + (new_y - y)**2) < tolerancia:\n",
    "            break\n",
    "            \n",
    "        # Actualizar x y y\n",
    "        x, y = new_x, new_y\n",
    "        \n",
    "    return [x, y]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$. \n",
    "  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Gradiente:  [256 256]\n",
      "Matriz Hessiana:  [[192   0]\n",
      " [  0 192]]\n",
      "Determinante de la Matriz Hessiana:  36863.99999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir las variables\n",
    "x1 = 4\n",
    "x2 = 4\n",
    "\n",
    "# Calcular las derivadas para el vector gradiente\n",
    "df_dx1 = 4 * x1**3\n",
    "df_dx2 = 4 * x2**3\n",
    "\n",
    "# Calcular las segundas derivadas para la matriz Hessiana\n",
    "d2f_dx12 = 12 * x1**2\n",
    "d2f_dx22 = 12 * x2**2\n",
    "\n",
    "# Crear el vector gradiente y la matriz Hessiana\n",
    "gradiente = np.array([df_dx1, df_dx2])\n",
    "hessiana = np.array([[d2f_dx12, 0], [0, d2f_dx22]])\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"Vector Gradiente: \", gradiente)\n",
    "print(\"Matriz Hessiana: \", hessiana)\n",
    "\n",
    "# Calcular el determinante de la matriz Hessiana\n",
    "det_hessiana = np.linalg.det(hessiana)\n",
    "print(\"Determinante de la Matriz Hessiana: \", det_hessiana)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Trabajo_Práctico_1_Matemática_para_Ciencia_de_los_Datos.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
